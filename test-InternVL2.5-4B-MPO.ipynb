{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a367219-a2e5-4c1b-90d7-4e2236ab9e85",
   "metadata": {},
   "source": [
    "### InternVL2_5-4B-MPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c278ef-8067-484e-8b16-b3db7c97ec36",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 모델, 토크나이저, 질의 데이터 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182548b-f838-4e78-bd30-e04d25329b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "# If you want to load a model using multiple GPUs, please refer to the `Multiple GPUs` section.\n",
    "model_id = 'OpenGVLab/InternVL2_5-4B-MPO'\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, use_fast=False)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f063fc-9bee-487b-8fba-0a9ab5fe89cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "dataset_name = 'KoAlpaca_v1.1.jsonl'\n",
    "data_files = {\"total\": dataset_name}\n",
    "\n",
    "dataset = load_dataset(path=os.getcwd(), data_files=data_files)\n",
    "total_data = dataset['total']\n",
    "train_dataset, eval_dataset = total_data.train_test_split(train_size=0.05, seed=42).values()\n",
    "total_data = train_dataset.remove_columns('url')\n",
    "\n",
    "def Tokenize_function(example):\n",
    "    return tokenizer(example['instruction'], example['output'], truncation=True)\n",
    "\n",
    "tokenized_data = total_data.map(Tokenize_function, batched=True)\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_data,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c67822-58e3-4b1a-adcb-e5c2d91d002b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 질의 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2029631-59ad-4bbe-96f5-5f2a9455801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "english_1_url = \"english_1_a_1.png\"\n",
    "english_2_url = \"english_2_a_4.png\"\n",
    "english_3_url = \"english_3_a_2.png\"\n",
    "\n",
    "math_1_url = \"math_1_a_1.png\" \n",
    "math_2_url = \"math_2_a_5.png\"\n",
    "math_3_url = \"math_3_a_5.png\"\n",
    "\n",
    "korean_1_url = \"korean_1_a_1.png\" \n",
    "korean_2_url = \"korean_2_a_1.png\"\n",
    "korean_3_url = \"korean_3_a_2.png\"\n",
    "\n",
    "english_1_url_image = Image.open(english_1_url)\n",
    "english_2_url_image = Image.open(english_2_url)\n",
    "english_3_url_image = Image.open(english_3_url)\n",
    "\n",
    "math_1_url_image = Image.open(math_1_url)\n",
    "math_2_url_image = Image.open(math_2_url)\n",
    "math_3_url_image = Image.open(math_3_url)\n",
    "\n",
    "korean_1_url_image = Image.open(korean_1_url)\n",
    "korean_2_url_image = Image.open(korean_2_url)\n",
    "korean_3_url_image = Image.open(korean_3_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b33ba1-dad2-4841-9e27-091b1c0c4155",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 영어 질의 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2adced-6372-4f2f-bdd3-19f9f38bdc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 텍스트 추출\n",
    "# 동작 시간 측정 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image(english_1_url, max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# single-image multi-round conversation\n",
    "question = '<image>\\nExtract words'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4661142-81ec-4381-994b-e3cda6fa754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 텍스트 추출\n",
    "# 동작 시간 측정 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image(english_1_url, max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=2048, do_sample=True)\n",
    "\n",
    "# single-image multi-round conversation\n",
    "question = '<image>\\nProvide an explanation of the correct answer to the question. Please speak korean.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5571f3-7b08-47aa-9fb5-62b6b218ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 텍스트 추출\n",
    "# 동작 시간 측정 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image(english_2_url, max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=2048, do_sample=True)\n",
    "\n",
    "# single-image multi-round conversation\n",
    "question = '<image>\\nProvide an explanation of the correct answer to the question. Please speak korean.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a1e973-5284-402f-bc06-747b4a01c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 텍스트 추출\n",
    "# 동작 시간 측정 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image(english_3_url, max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=2048, do_sample=True)\n",
    "\n",
    "# single-image multi-round conversation\n",
    "question = '<image>\\nProvide an explanation of the correct answer to the question. Please speak korean.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96634b7a-f66e-462a-afa6-9e1288a47afb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 수학 질의 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba734bdc-6c04-4468-8171-48cbffb5cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 텍스트 추출\n",
    "# 동작 시간 측정 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image(math_1_url, max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=2048, do_sample=True)\n",
    "\n",
    "# single-image multi-round conversation\n",
    "question = '<image>\\nProvide an explanation of the correct answer to the question. Please speak korean.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb5355-7b33-4ca3-93a9-57fc7a157915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 텍스트 추출\n",
    "# 동작 시간 측정 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image(math_2_url, max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# single-image multi-round conversation\n",
    "question = '<image>\\nProvide an explanation of the correct answer to the question. Please speak korean.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2350e70-0a89-4feb-b14c-abc67545f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 텍스트 추출\n",
    "# 동작 시간 측정 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image(math_3_url, max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# single-image multi-round conversation\n",
    "question = '<image>\\nProvide an explanation of the correct answer to the question. Please speak korean.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329bc02-9bf8-4c57-8fa1-e2487f709ec3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 국어 질의 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad103a1d-cf87-4b8f-ac39-61f16fc087aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 텍스트 추출\n",
    "# 동작 시간 측정 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image(korean_3_url, max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# single-image multi-round conversation\n",
    "question = '<image>\\nExtract words'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93388cf4-6d03-4942-b34e-534f26d72b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 텍스트 추출\n",
    "# 동작 시간 측정 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image(korean_1_url, max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=2048, do_sample=True)\n",
    "\n",
    "# single-image multi-round conversation\n",
    "question = '<image>\\nProvide an explanation of the correct answer to the question. Please speak korean.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bb1dc-efcf-4a24-8b31-c0d9d158b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 텍스트 추출\n",
    "# 동작 시간 측정 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image(korean_2_url, max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=2048, do_sample=True)\n",
    "\n",
    "# single-image multi-round conversation\n",
    "question = '<image>\\nProvide an explanation of the correct answer to the question. Please speak korean.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c5ab3-0843-40e5-9a79-62999e2238be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 텍스트 추출\n",
    "# 동작 시간 측정 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image(korean_3_url, max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=2048, do_sample=True)\n",
    "\n",
    "# single-image multi-round conversation\n",
    "question = '<image>\\nProvide an explanation of the correct answer to the question. Please speak korean.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_lightning]",
   "language": "python",
   "name": "conda-env-torch_lightning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
